---
title: "mlr_assg4"
output: html_document
date: "2022-09-20"
---

```{r setup, include=FALSE}
library(cowplot)
library(ggplot2)
library(reshape2)
library(grid)
library(MASS)
```


```{r crimes}
crime_data<-read.table(file = "./uscrime.txt", sep = "\t",header=TRUE)
head(crime_data)
dim(crime_data)
summary(crime_data)
```

#### Data Distribution and Pairwise Corelation
### Data Distribution
```{r}
my_plots <- lapply(names(crime_data), function(var_x){
  p <- 
    ggplot(crime_data) +
    aes_string(var_x)

  if(var_x %in% list("So")) {
    p <- p + geom_bar()

  } else {
    p <- p + geom_density()
  } 

})

plot_grid(plotlist = my_plots)

### Correlation
cormat <- round(cor(crime_data),2)
cormat[upper.tri(cormat)] <- NA
melted_cormat <- melt(cormat)
# plotting the correlation heatmap
library(ggplot2)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2,
                                   fill=value)) +
geom_tile() +
geom_text(aes(Var1, Var2, label = value),
          color = "white", size = 3)
```
## Observations: 
Wealth and Income Inequality are 2 highly correlated variables (-ve correlation) ,hence we can pick only 1 variable out of the 2
Po1 and Po2 are 2 correlated variables , hence we can pick only 1 out of 2 
Same goes for U1 and U2, , hence we can pick only 1 out of 2 

The dependent variable Crime Rate has high correlation with the following variables :
Po1 and Po2 -> we see a surprising +ve correlation with crime rate , As regression cannot be used for defining a causal relation , we can infer the other way round that since the crime rate is higher , that could be the reason of a higher per capita expenditure on police protection.

Prob -> makes sense to have -ve correlation in this case . As the crime rate is bound to decrease if probability of imprisonment increases.


```{r}
options(scipen=999)
crime_data$So<-as.factor(crime_data$So)
# Linear Regression model with all features
lmodel_1<-lm(Crime~M+So+Ed+Po1+Po2+LF+M.F+Pop+NW+U1+U2+Wealth+Ineq+Prob+Time,data=crime_data)
summary(lmodel_1)
# plotting the residuals to check for assumptions in the plain vanilla model with all features
par(mfrow = c(2, 2))
plot(lmodel_1)


new_datapoint<-data.frame(M=c(14.0),So=c(as.factor(0)),Ed=c(10.0),Po1=c(12.0),Po2=c(15.5),LF=c(0.640),M.F=c(94.0),Pop=c(150),NW=c(1.1),U1=c(0.120),U2=c(3.6),Wealth=c(3200),Ineq=c(20.1),Prob=c(0.04),Time=c(39.0))
crime_rate_pred<-predict(lmodel_1,newdata = new_datapoint)
crime_rate_pred<-round(crime_rate_pred,2)
print(paste("Predicted Value:", crime_rate_pred))
```
# Interpreting the output of the linear regression model : 
Based on the model output , it appears that the features Ed, Ineq, Prob,M , Po1 are significant variables based on the p-values . \
* The lower the p-value , the more significant the variable  because the p-value signifies the probability of the estimated coefficients $\hat{\beta_{i}}'s$ coming from the underlying assumed distribution of null hypothesis (Null Hypothesis states that $\beta_{i}'s=0$). Thus , lower the p-value , higher the probability that the coefficient of the variable is not equal to 0 and hence that variable is a good predictor.\

* For this model , the $R^{2}$ value is 0.8 which means that 80% of the variance in the dependent variable is explained by the set of these independent variables .\

* The predicted crime_rate for the given data point is `r crime_rate_pred`.\

* From the 4 diagnostic plots associated with the linear regression model , we can see that the assumptions are mostly in place although there are some slight deviations, error terms are normally distributed (deviation for a couple of points) and residuals are mostly homogeneous around the fitted line. However , we can see slight bends in the left and right end in the plot of residuals vs fitted values. Thus , a possible remedy can be thought of as modelling the log of dependent variable based on independent variable. (because taking a log transformation reduces the range of possible variables.)

# Points for further exploration  : 
* Taking a log transformation of the dependent variable and then create a regression model.
* Create a model with lower number of features (remove highly correlated features).
* Look for possible outliers/leverage points (based on the residuals vs leverage plot - cook's distance) and remove them to see changes in the model output. 
* We can use Box-Cox transformation to identify the best possible transformation of y to regress on . This is usually done to handle the points deviating a bit from the normal Q-Q plot.

```{r}
# Box Cox Transformation (we can try this out to find the best possible function of y to regress on , the value of lambda is determined through MLE)
bc<-boxcox(lmodel_1, lambda =seq(-3,3))
#extract the best lambda 
best.lam<-bc$x[which(bc$y==max(bc$y))]

```
# Observations: 
Since , the above graph gives us the best value of $\lambda$ to be around 0 only , therefore taking a log transformation of the dependent variable y makes more sense. 

```{r}
# Taking log transformation of the dependent variable with all features
options(scipen=999)
crime_data$So<-as.factor(crime_data$So)
# Linear Regression model with all features
lmodel_2<-lm(log(Crime)~M+So+Ed+Po1+Po2+LF+M.F+Pop+NW+U1+U2+Wealth+Ineq+Prob+Time,data=crime_data)
summary(lmodel_2)
# plotting the residuals to check for assumptions in the plain vanilla model with all features
par(mfrow = c(2, 2))
plot(lmodel_2)

new_datapoint<-data.frame(M=c(14.0),So=c(as.factor(0)),Ed=c(10.0),Po1=c(12.0),Po2=c(15.5),LF=c(0.640),M.F=c(94.0),Pop=c(150),NW=c(1.1),U1=c(0.120),U2=c(3.6),Wealth=c(3200),Ineq=c(20.1),Prob=c(0.04),Time=c(39.0))
crime_rate_pred<-predict(lmodel_2,newdata = new_datapoint)
crime_rate_pred<- exp(crime_rate_pred)
print(paste("Predicted Value:", round(crime_rate_pred,2)))
```
Although the $R^{2}$ takes a slight dip , but it appears that the model assumptions are better satisfied here .\ 
Points 11,19,46 seems to have around 1.5*standard deviation , thus these could be tested as potential outliers/influential points.

```{r}
# Taking log transformation of the dependent variable with all features
options(scipen=999)
crime_data$So<-as.factor(crime_data$So)
# Linear Regression model with reduced # of features
lmodel_3<-lm(log(Crime)~M+So+Ed+Po1+LF+M.F+Pop+NW+U1+Ineq+Prob+Time,data=crime_data)
summary(lmodel_3)
# plotting the residuals to check for assumptions in the plain vanilla model with all features
par(mfrow = c(2, 2))
plot(lmodel_3)

new_datapoint<-data.frame(M=c(14.0),So=c(as.factor(0)),Ed=c(10.0),Po1=c(12.0),Po2=c(15.5),LF=c(0.640),M.F=c(94.0),Pop=c(150),NW=c(1.1),U1=c(0.120),U2=c(3.6),Wealth=c(3200),Ineq=c(20.1),Prob=c(0.04),Time=c(39.0))
crime_rate_pred<-predict(lmodel_3,newdata = new_datapoint)
crime_rate_pred<- exp(crime_rate_pred)
print(paste("Predicted Value:", round(crime_rate_pred,2)))
```


```{r}
# removing the potential outliers/influential points based on the diagnostic plots of linear regression model
crime_data_wo<-crime_data[-c(11, 19, 46),]

# Taking log transformation of the dependent variable with all features
options(scipen=999)
crime_data$So<-as.factor(crime_data$So)
# Linear Regression model with reduced # of features and removed potential influential points.
lmodel_4<-lm(log(Crime)~M+So+Ed+Po1+LF+M.F+Pop+NW+U1+Ineq+Prob+Time,data=crime_data)
summary(lmodel_4)
# plotting the residuals to check for assumptions in the plain vanilla model with all features
par(mfrow = c(2, 2))
plot(lmodel_4)

new_datapoint<-data.frame(M=c(14.0),So=c(as.factor(0)),Ed=c(10.0),Po1=c(12.0),Po2=c(15.5),LF=c(0.640),M.F=c(94.0),Pop=c(150),NW=c(1.1),U1=c(0.120),U2=c(3.6),Wealth=c(3200),Ineq=c(20.1),Prob=c(0.04),Time=c(39.0))
crime_rate_pred<-predict(lmodel_4,newdata = new_datapoint)
crime_rate_pred<- exp(crime_rate_pred)
print(paste("Predicted Value:", round(crime_rate_pred,2)))

```