
```{r}
library(cowplot)
library(ggplot2)
library(reshape2)
library(grid)
library(MASS)
library(rpart)
library(rpart.plot)
library(mlr)
```

```{r,include=FALSE}
#install.packages("mlr")
```

```{r crimes}
crime_data<-read.table(file = "../uscrime.txt", sep = "\t",header=TRUE)
head(crime_data)
dim(crime_data)
summary(crime_data)
```
```{r}
### Data Distribution and Pairwise Corelation

my_plots <- lapply(names(crime_data), function(var_x){
  p <- 
    ggplot(crime_data) +
    aes_string(var_x)

  if(var_x %in% list("So")) {
    p <- p + geom_bar()

  } else {
    p <- p + geom_density()
  } 

})

plot_grid(plotlist = my_plots)

### Correlation
cormat <- round(cor(crime_data),2)
cormat[upper.tri(cormat)] <- NA
melted_cormat <- melt(cormat)
# plotting the correlation heatmap

ggplot(data = melted_cormat, aes(x=Var1, y=Var2,
                                   fill=value)) +
geom_tile() +
geom_text(aes(Var1, Var2, label = value),
          color = "white", size = 3)
```

# Training and evaluating on the same training dataset (not a good method for generalizing) , better to do cross validation 
```{r}
# Vanilla decision tree with a fixed value of complexity parameter of 0.01 and minsplit of 20
reg_tree<-rpart(Crime~.,data=crime_data,method="anova",control=c(cp=0.01,minsplit=20))
rmse<-sqrt(sum((crime_data$Crime - predict(reg_tree,type=c("vector")))**2)/nrow(crime_data))
rmse
```

```{r}
rpart.plot(reg_tree)
```

* Observations & Points for further exploration: 

+ The above decision tree uses 3 variables NW , Pop, LF and Po1 to create a decision tree. (2 of them Pop and Po1 have a linear correlation with dependent variable i.e. Crime)

+ One food for thought is that this is a regression tree and we are imputing the mean values of all the points that fall in a leaf or terminal node , so incase if we have an outlier , then those will drive the value of mean and hence cause higher RMSE or SSE. So , it would be good to see the effect of removing or capping outliers and then performing decision tree.

+ The smaller the value of complex parameter , the model is expected to have high variance as a split would be performed if we get such a marginal increase in entropy . Therefore , to prevent our model from overfitting amd having high variance , it is better if we not choose too small values of cp i.e. complexity parameter.

+ The default value of max depth parameter here is 30 but given that we have only 47 nodes in our dataset , it dfoes not make much sense to have tree models greater than or equal to 3 or 4 levels of depth 

+ minsplit hyperparameter : The default value of this parameter is 20 i.e. a split would be performed if we have atleast 20 datapoints in a node , although it is generally good to have higher values of "minsplit" parameter to avoid overfitting but since in this problem statement , the total # of datapoints is only 47 , so this is one parameter where we can tweak in a bit .

+ minbucket hyperparameter : Similarly , this hyperparameter means  the number of datpoints in a terminal node and thus here also , it makes sense to have a smaller value for this hyperparameter as we do not have large dataset for training our model.

#Perform hyperparameter tuning minsplit and cp(complexity hyperparameter)
```{r}
folds <- sample(rep(1:5, length.out = nrow(crime_data)), size = nrow(crime_data), replace = F)
pred_table<-data.frame(min_split=numeric(0),cp=double(0),rmse=double(0))
for(j in seq(0.001,0.02,0.001)){
for(i in seq(5,25,2)){
  CV_mse <- lapply(1:5, function(x){ #5 corresponds to the number of folds defined earlier
  model <- rpart(Crime ~ ., data = crime_data[folds != x,],method="anova",control=c(minsplit=i,cp=j))
  preds <- predict(model,  crime_data[folds == x,], type="vector")
  sum_sse<-sum((crime_data[folds==x,"Crime"]-preds)**2)
  rmse<-sqrt(sum_sse/nrow(crime_data[folds==x,]))
  #total_rmse<-total_rmse+rmse
  return(rmse)
   })
pred_table[nrow(pred_table)+1,]=c(i,j,mean(unlist(CV_mse)))
}
}  
#rpart.plot(rpart(Crime ~ ., data = crime_data,method="anova",minsplit=10,cp=0.01))
#plot(pred_table)
```


# Identifying the optimal value of the hyperparameters cp and min_split
```{r}
options(scipen=999)
pred_table[which.min(pred_table$rmse),c("min_split","cp")]
```
* From the above output , we get the the optimal value of cp as 0.001 and min_split as 15
```{r}
reg_cv_tree<-rpart(Crime~.,data=crime_data,method="anova",control=c(cp=0.001,minsplit=15))
rpart.plot(reg_cv_tree)
```


* Note: The above set of optimal parameters reproduces the same decision tree that we had done initially with fixed values of these 2 parameters.This can be because of less # of datapoints. We can try reducing the number of minsplits which will lead to a higher variance model and we can try iterating to get the optimal maxDepth hyperparameter.

```{r}
pred_table<-data.frame(min_split=numeric(0),maxDepth=double(0),rmse=double(0))
for(j in seq(2,15,1)){
for(i in seq(5,25,2)){
  CV_mse <- lapply(1:5, function(x){ #5 corresponds to the number of folds defined earlier
  model <- rpart(Crime ~ ., data = crime_data[folds != x,],method="anova",control=c(minsplit=i,maxdepth=j))
  preds <- predict(model,  crime_data[folds == x,], type="vector")
  sum_sse<-sum((crime_data[folds==x,"Crime"]-preds)**2)
  rmse<-sqrt(sum_sse/nrow(crime_data[folds==x,]))
  #total_rmse<-total_rmse+rmse
  return(rmse)
   })
pred_table[nrow(pred_table)+1,]=c(i,j,mean(unlist(CV_mse)))
}
}  
#rpart.plot(rpart(Crime ~ ., data = crime_data,method="anova",minsplit=10,cp=0.01))
#plot(pred_table)
```

* Identifying the optimal value of the hyperparameter maxDepth (which comes out to be around 2 with min_split within range of 5-15 (rmse is same for different values of min_split))
```{r}
options(scipen=999)
pred_table[which.min(pred_table$rmse),c("maxDepth")]
```

```{r}
reg_tree_final<-rpart(Crime~.,data=crime_data,method="anova",control=c(cp=0.001,minsplit=15,maxdepth=2))
rmse_final<-sqrt(sum((crime_data$Crime - predict(reg_tree_final,type=c("vector")))**2)/nrow(crime_data))
rmse_final
```
#Conclusion : The RMSE value for the decision tree model using the optimal values of the hyperparameter is `r round(rmse_final,2)`


