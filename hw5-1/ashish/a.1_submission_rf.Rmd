---
title: "Random Forest | Ques 10.1"
author: "Ashish Dhiman"
date: "2022-09-26"
output: html_document
---

### Read Data

```{r, warning=FALSE}
set.seed(77)
library(randomForest)
library(ggplot2)

crime_df = read.table("../uscrime.txt", sep="\t", header= TRUE)
dim(crime_df)
```

#### Data and variable meaning:

Criminologists are interested in the effect of punishment regimes on crime rates. This has been studied using aggregate data on 47 states of the USA for 1960.

| Variable | Description                                                                            |
|--------------------|----------------------------------------------------|
| M        | percentage of males aged 14--24 in total state population                              |
| So       | indicator variable for a southern state                                                |
| Ed       | mean years of schooling of the population aged 25 years or over                        |
| Po1      | per capita expenditure on police protection in 1960                                    |
| Po2      | per capita expenditure on police protection in 1959                                    |
| LF       | labour force participation rate of civilian urban males in the age-group 14-24         |
| M.F      | number of males per 100 females                                                        |
| Pop      | state population in 1960 in hundred thousands                                          |
| NW       | percentage of nonwhites in the population                                              |
| U1       | unemployment rate of urban males 14--24                                                |
| U2       | unemployment rate of urban males 35--39                                                |
| Wealth   | wealth: median value of transferable assets or family income                           |
| Ineq     | income inequality: percentage of families earning below half the median income         |
| Prob     | probability of imprisonment: ratio of number of commitments to number of offenses      |
| Time     | average time in months served by offenders in state prisons before their first release |
| Crime    | crime rate: number of offenses per 100,000 population in 1960                          |

```{r}
crime_df$So <- factor(crime_df$So)
```

### Build Vanilla RF model

```{r}
rf_model_v0 = randomForest(Crime ~ ., data = crime_df, importance=TRUE)
summary(rf_model_v0)

rmse_func = function(true,predicted){
  (mean((true-predicted)^2))**0.5
}

rmse_func(true = crime_df$Crime, predicted=rf_model_v0$predicted)

varImpPlot(rf_model_v0,sort = TRUE)
```

From variable importance we can see that the some variables like, So, U1 and U2 are not very helpful. Hence it might serve us better to drop these variables, bcos they might add more noise to our model instead of actual information capture. In the CV stage below, we will test the models without these variables.

```{r}
rf_model_v1 = randomForest(Crime ~ . -U1 -M.F -U2, data = crime_df, importance = TRUE)
varImpPlot(rf_model_v1)
```

### Build RF and tune hyperparams with CV

#### Build CV folds

```{r}
n_folds = 5
folds <- sample(rep(1:n_folds, length.out = nrow(crime_df)), size = nrow(crime_df), replace = F)

table(folds)
```

We have created uniform CV folds here

#### Function for RF on CV fold

```{r}
CV_rf_func = function(test_fold, ntree1, mtry1){
  model1 <- randomForest(Crime ~ ., data = crime_df[folds != test_fold,],
                        ntree = ntree1, mtry=mtry1)
  model2 <- randomForest(Crime ~ . -U1 -M.F -U2, data = crime_df[folds != test_fold,],
                        ntree = ntree1, mtry=mtry1)
  model3 <- randomForest(Crime ~ . -U1 -U2 -Time, data = crime_df[folds != test_fold,],
                        ntree = ntree1, mtry=mtry1)
  
  preds1 <- predict(model1,  crime_df[folds == test_fold,])
  preds2 <- predict(model2,  crime_df[folds == test_fold,])
  preds3 <- predict(model3,  crime_df[folds == test_fold,])
  
  rmse1 = rmse_func(true = crime_df[folds == test_fold,"Crime"],predicted = preds1)
  rmse2 = rmse_func(true = crime_df[folds == test_fold,"Crime"],predicted = preds2)
  rmse3 = rmse_func(true = crime_df[folds == test_fold,"Crime"],predicted = preds3)
  
  return(c(rmse1,rmse2,rmse3))
}

```

```{r, warning=FALSE, message=FALSE}
rmse_list1=c()
rmse_list2=c()
rmse_list3=c()
ntree_list=c()
mtry_list=c()

for (ntree_i in seq(30,130,10)) {
  for (mtry_i in seq(2,6,1)) {
    #RMSE on each CV Fold for ntree_i,mtry_i
    rmse_cv_all = lapply(seq(1:5), function(fold) 
      CV_rf_func(test_fold = fold, ntree1 = ntree_i, mtry1 = mtry_i))
    rmse_cv_df = do.call(rbind.data.frame, rmse_cv_all)
    rmse_cv1 = mean(unlist(rmse_cv_df[,1]))
    rmse_cv2 = mean(unlist(rmse_cv_df[,2]))
    rmse_cv3 = mean(unlist(rmse_cv_df[,3]))
    print (paste("For tree:",ntree_i,"mtry:",mtry_i))
    rmse_list1 = c(rmse_list1, rmse_cv1)
    rmse_list2 = c(rmse_list2, rmse_cv2)
    rmse_list3 = c(rmse_list3, rmse_cv3)
    ntree_list = c(ntree_list, ntree_i)
    mtry_list = c(mtry_list, mtry_i)
}
}
```

```{r}
rmse_df = cbind.data.frame(ntree_list,mtry_list,
                           rmse_list1,rmse_list2,rmse_list3)
names(rmse_df)=c("ntree","mtry","rmse_model1","rmse_model2","rmse_model3")
```

```{r}
rmse_df
```

### Let us study the impact of these two hyper-parameters on RMSE

#### Impact of ntree

```{r}
ggplot(rmse_df[rmse_df$mtry<4,], aes(x = ntree, y = rmse_model1, group = mtry, color = mtry)) +
  geom_point() + geom_line()


ggplot(rmse_df[rmse_df$mtry<4,], aes(x = ntree, y = rmse_model2, group = mtry, color = mtry)) +
  geom_point() + geom_line()

ggplot(rmse_df[rmse_df$mtry<4,], aes(x = ntree, y = rmse_model3, group = mtry, color = mtry)) +
  geom_point() + geom_line()
```

Because our data is so small; the effect of ntree and mtry is very sporadic, with high interaction between them. Also the trend varies as per the choice of the model(i.e the choice of predictors).

Ideally we would have expected the RMSE to exhibit a 'U' shaped trend with out hyper parameters, where adding complexity to model (i.e. more num of trees to average upon or more features to split nodes upon) first decreases RMSE, but after a point it starts overfitting, and RMSE starts increasing again.

```{r}
print ("Min RMSE model 1")
rmse_df[which.min(rmse_df[,"rmse_model1"]),]

print ("Min RMSE model 2")
rmse_df[which.min(rmse_df[,"rmse_model2"]),]

print ("Min RMSE model 3")
rmse_df[which.min(rmse_df[,"rmse_model3"]),]
```

As

### Best Model is therefore given for:

1.  Dropping variables: U1, M.F and U2
2.  number of trees = 40
3.  mtry = 4

Not only thus by RMSE, but by intution too this is a simpler model since it has lesser features, lower number of trees, as well as lower number of features to split on in RF.

Using the above hyper-parameters we can build the best model on full data.

```{r}
rf_final_model <- randomForest(Crime ~ . -U1 -M.F -U2, data = crime_df, ntree = 40, mtry=4, importance = TRUE)
rmse_func(true=crime_df$Crime,predicted = rf_final_model$predicted)
```

```{r}
varImpPlot(rf_final_model)
```

From the variable importance plot we see, that the following variable feature very highly in Importance:

1.  Po2
2.  NW
3.  Prob

### Comparison between Decision Tree and Random Forest

Note: while the above 3 variables are also significant features here is the variable importance of variable "Pop". While this variable is very important for Decision trees, it is not that important for Random Forest. One possible reason for this could be the notion of randomness involved in RF.

Another point to note here is that after Hyperparameter tuning, Decision tree is giving lower RMSE, and is most likely over-fitting the data-set, in comparison to Random Forest, and if we had a test data, RF model would most likely perform better, because of better generalization.
