---
title: "Submission HW1"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

# ISYE 6501 \| Fall 22

-   Ashish Dhiman \| [ashish.dhiman\@gatech.edu](mailto:ashish.dhiman@gatech.edu){.email}

-   Abhinav Arun \| [aarun60\@gatech.edu](mailto:aarun60gatech.edu){.email}

-   Anshit Verma \| [averma373\@gatech.edu](mailto:averma373@gatech.edu){.email}

[**\*Analysis Notes are marked with Red header: #Analysis**]{style="color:red"}

## Question 2.1

### Example 1: Credit Risk Evaluation

Classification models find use cases across a spectrum of Credit Risk functions. A typical example being classifying a particular transaction as risky or otherwise basis which it is either declined or authorized. Here risk implies that the individual might not be able to make the required payments in the future, and therefore transactions made are riskier.

**Objective Function (**$Y$): To classify individual transactions as risky (decline) or not.\
**Predictor Variables (**$X_i$): Some of the key predictor variables given below:

-   ***Past Delinquency***: Delinquency implies that the individual was unable to keep up on his monthly payments previously and missed on his obligated payments. This is a key marker for credit risk, and past delinquent behavior hints towards potential future risk.

-   ***Credit Utilization***: Credit Utilization is defined as the ratio of current balance to overall credit limit accorded to the individual. Suppose a individual has a credit line of \$10,000 and he already has utilized \$ 9k of it. This individual is generally prone to more risk as compared to the individual who only has utilized say \$ 2k of his \$ 10k line.

-   ***Current Debt to Income ratio***: This is a measure of Income to Debt Capacity of the individual. In other words, it stacks up the overall debt obligations of the individual across mortgage, auto loan, credit cards etc., against his total income. If a larger part of an individual's income is directed towards his debt payment, than he/she again might be more susceptible to miss payments in the future and hence is riskier.

-   ***Amount of Transaction***: This is the dollar amount of the transaction. Intuitively a transaction of \$20k is riskier than \$5, since even if the individual misses his payment, the hit taken by the firm is restricted to only \$5.

## Question 2.2

### Exploratory Data Analysis (EDA)

```{r}
#imports
library(cowplot)
library(ggplot2)
library(reshape2)
```

#### Read Data and Summary

```{r}
org_cc_data <- read.table(file = './data 2.2/credit_card_data-headers.txt', sep = "\t", header = TRUE)
dim(org_cc_data)
head(org_cc_data)
summary(org_cc_data)
```

[**#Analysis:**]{style="color:red"}

**A1,A9,A10,A12 are binary basis min/max values, rest are continuous**

**For target variable mean is \~45% ==\> variable is not grossly imbalanced**

#### Data Distribution and Co relation

```{r}
### Data Distribution
my_plots <- lapply(names(org_cc_data), function(var_x){
  p <- 
    ggplot(org_cc_data) +
    aes_string(var_x)

  if(var_x %in% list("A1","A9","A10","A12","R1")) {
    p <- p + geom_bar()

  } else {
    p <- p + geom_density()
  } 

})

plot_grid(plotlist = my_plots)


### Corealtion
cormat <- round(cor(org_cc_data),2)
cormat[upper.tri(cormat)] <- NA
melted_cormat <- melt(cormat)
# plotting the correlation heatmap
library(ggplot2)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2,
                                   fill=value)) +
geom_tile() +
geom_text(aes(Var1, Var2, label = value),
          color = "white", size = 3)
```

[**#Analysis:**]{style="color:red"}

**If \|cor\| \> 0.3 ==\> Significant, then following pairs show high correlation:**

-   R1: A8,A9,A10,A11 (We expect these to show up with high weights in SVM eqn.)

-   A11: A8,A9,A10

-   A10: A9

-   A9: A8

-   A8: A2

## Question 2.2 (part 1: kSVM with Linear kernel)

```{r}
#imports
library(kernlab)
library(ggplot2)
```

```{r}
dim(org_cc_data)
names(org_cc_data)
```

### v0: Vanilla Model and Accuracy function

```{r}
model_v0 = ksvm(x=as.matrix(org_cc_data[,1:10]), y=org_cc_data[,11], scaled =TRUE, type = "C-svc",kernel = "vanilladot", C = 10)

### Accuracy function: Overall and in each class
acc_func <- function(model) {
   pred_all <- predict(model,org_cc_data[,1:10])
   print (paste("Overall Acc:", round(sum(pred_all == org_cc_data[,11]) * 100 / nrow(org_cc_data),4)))
   
   pred_1 <- predict(model,org_cc_data[org_cc_data$R1 == 1,1:10])
   print (paste("Acc in 1's:", round(sum(pred_1 == org_cc_data[org_cc_data$R1 == 1,11]) * 100 / nrow(org_cc_data[org_cc_data$R1 == 1,1:10]),4)))
   
   pred_0 <- predict(model,org_cc_data[org_cc_data$R1 == 0,1:10])
   print (paste("Acc in 0's:", round(sum(pred_0 == org_cc_data[org_cc_data$R1 == 0,11]) * 100 / nrow(org_cc_data[org_cc_data$R1 == 0,1:10]),4)))
}

acc_func(model_v0)
print(paste("#Support Vectors",model_v0@nSV))
```

[**#Analysis:**]{style="color:red"} \*\*There are 190 support vectors, or roughly 30% of the total data points.

### v1: Optimise C

```{r}
C_values <- c(0.0001,0.001,0.0015,0.002,0.005,0.01,0.03) #Range identified with hit and trial

for (C_i in C_values) {
  print (paste("For C = ",C_i))
  acc_func(ksvm(x=as.matrix(org_cc_data[,1:10]), y=org_cc_data[,11], scaled =TRUE, type = "C-svc",kernel = "vanilladot", C = C_i))
  print("")
}
```

[**#Analysis:**]{style="color:red"} **With increasing C, we weight the cost function more towards misclassification (realtive to margin). Therefore intuitively incereasing C, should increase Accuracy albeit at the cost of margin. And in the above data points too, similar effect is apparent**

**C=0.015 seems the best option, because even with almost equal accuracy, the accuracy in the negative class is higher, where for other C values the accuracy amongst classes is more lopsided**

### Final SVM Model and it's equation

$W = \sum_i(\alpha Y_i X_{support\_vector_i})$

```{r}
modelf = ksvm(x=as.matrix(org_cc_data[,1:10]), y=org_cc_data[,11], scaled =TRUE, type = "C-svc",kernel = "vanilladot", C = 0.0015)
acc_func(modelf)

a <- colSums(modelf@xmatrix[[1]] * modelf@coef[[1]])
a0 <- -modelf@b
a
a0
```
[**#Analysis:**]{style="color:red"}

## Question 2.2 (part 2: kSVM with Non Linear kernel)

We have tried 2 kernels:

1.  Radial Basis, and

2.  Laplace

### Non Linear models: Radial Basis Kernel
```{r}
C_values <- c(0.01,1,10,50,100,1000,2000)

for (C_i in C_values) {
  print (paste("For C = ",C_i))
  modeli = ksvm(x=as.matrix(org_cc_data[,1:10]), y=org_cc_data[,11], scaled =TRUE, type = "C-svc",kernel = "rbfdot", C = C_i)
  acc_func(modeli)
  print(modeli@nSV)
}
```

### Non Linear models: Laplace Kernel
```{r}
C_values <- c(0.01,1,10,50,100,1000,2000)

for (C_i in C_values) {
  print (paste("For C = ",C_i))
  modeli = ksvm(x=as.matrix(org_cc_data[,1:10]), y=org_cc_data[,11], scaled =TRUE, type = "C-svc",kernel = "laplacedot", C = C_i)
  acc_func(modeli)
  print(modeli@nSV)
}
```