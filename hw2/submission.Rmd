---
title: "ISYE 6501 | Submission HW2"
output:
  html_document:
    df_print: paged
  pdf_document: default
date: "2022-09-04"
---

# ISYE 6501 \| Fall 22

-   Ashish Dhiman \| [ashish.dhiman\@gatech.edu](mailto:ashish.dhiman@gatech.edu){.email}

-   Abhinav Arun \| [aarun60\@gatech.edu](mailto:aarun60gatech.edu){.email}

-   Anshit Verma \| [averma373\@gatech.edu](mailto:averma373@gatech.edu){.email}

[**\*Analysis Notes are marked with Red header: #Analysis**]{style="color:red"}

```{r}

set.seed(61)

# imports
library(ggplot2)
library(cowplot)
library(reshape2)
#library(factoextra)
library(kknn)
library(pROC)
```

## Read Data

```{r}
org_cc_data <- read.table(file = './data 3.1/credit_card_data-headers.txt', sep = "\t", header = TRUE)
dim(org_cc_data)
```

## Question 3.1

### Part 1: Cross Validation for finding best knnn model

Here we perform 5 fold cross validation, and follow the following pseudo code:

1.  Divide the data into 5 equal parts

2.  Choose i'th part as test data and train knn model on the rest 4 parts

    1.  Find the optimal threshold using the ROC curve

    2.  With this optimal threshold find accuracy on test set

3.  Repeat step 2 but slide i to the next part

4.  When all parts are rotated into the test set, calculate the measure for accuracy of model:

    1.  Mean test accuracy across 5 folds

    2.  Min test accuracy across the 5 folds (optional)

**We repeat the above 4 steps for different k values, to find the best model(or choice of k)**

### Step 1:

```{r}
#Split org_data into 5 parts
df_copy = org_cc_data
df_copy$cross_validation_i <- sample(5, size = nrow(df_copy), replace = TRUE)

summary_df = aggregate(df_copy$R1,list(df_copy$cross_validation_i), 
          FUN = function(x) c(mean = mean(x), proportion = length(x)/nrow(df_copy)))
print.data.frame(summary_df)
```

[**#Analysis**]{style="color:red"}

The above summary shows the mean of R1 and relative size of each fold. Ideally for all the folds this should be 45% mean and 20% proportion, but we see minor deviations in individual folds due to random sampling.

```{r}
#Function to train model and return prediction probabilities on test
train_test_i = function(df_train,df_test, k0 = 5){
  f = R1~A1+A2+A3+A8+A9+A10+A11+A12+A14+A15
  model = kknn(f,train = df_train,test = df_test,k=k0,kernel="rectangular",scale = TRUE)
  return (fitted(model))
}

# decide best threshold
best_threshold = function(pred_prob, df_test=org_cc_data){
  myroc <- roc(df_test$R1,as.numeric(unlist(pred_prob)), smooth=FALSE)
  print(paste("AUC is",auc(myroc)))
  #plot(myroc, auc.polygon=TRUE)
  return (coords(myroc,x="best")["threshold"][[1]])
}

#Accuracy for best threshold
acc_func = function(y_act, pred_probs, thresh = 0.5){
  y_pred = lapply(pred_probs, function(x) if(x>=thresh) 1 else 0)
  y_pred = as.numeric(unlist(y_pred))
  acc_overall = sum(y_act == y_pred)/length(y_act)
  acc_overall = round(acc_overall,4)* 100
  return (acc_overall)
}
```

### Step 2

```{r}
#Train model and test on ith group
cv_function_i = function(df, test_index_choice,k_choice){
  print("")
  print (paste("test index for CV Fold",test_index_choice))
  #split data into train and test
  mask_test_i = df$cross_validation_i == test_index_choice
  df_train = df[!mask_test_i,]
  df_test = df[mask_test_i,]
  print (paste("Train,Test Size =",nrow(df_train),nrow(df_test)))
  #train and get probaility predictions on test model
  test_probabilities = train_test_i(df_train,df_test, k0 = k_choice)
  
  #get best threshold for test probabilities
  ti = best_threshold(test_probabilities, df_test = df_test)
  if (length(ti) >1){
    ti = ti[[1]] #select 1st when there are multiple optimal thresholds
  }
  print (paste("Best threshold is",ti))
  #get test accuracy
  y_act = df_test$R1
  #print (paste("Length of Test Probabilities",nrow(df_test),length(test_probabilities)))
  return (acc_func(y_act = y_act, pred_probs = test_probabilities, thresh = ti))
}
```

```{r}
#Test fucntion with a sample i
cv_function_i(df = df_copy, test_index_choice = 1, k_choice = 16)
```

### Step 3

```{r}
#CV function: pass k, return test accuracy for all folds
cv_function = function(df = df_copy,k_choice = 5){
  print("----------------------------------------------")
  print(paste("For k value",k_choice))
  cv_accuracy = lapply(seq(1,5), function(x) cv_function_i(df=df_copy,x,k_choice))
  print("----------------------------------------------")
  return (cv_accuracy)
}
```

```{r}
#Test function with a sample k
cv_accuracy = cv_function(df = df_copy,k_choice = 16)
```

```{r}
unlist(cv_accuracy)
mean(unlist(cv_accuracy))
```

The values match the above test run values

### Step 4: Iterate on different models(i.e on different values of k) and use CV for best model

Here we differentiate two knn models, only using the k hyperparameter

```{r, results='hide'}
#Iterate over k and get mean test accuracy

end_k = 77 #almost half of test set size

acc_df <- data.frame(matrix(ncol = 0, nrow = length(seq(3,end_k,2))))
acc_df$k_kknn = seq(3,end_k,2)
mean_cv_acc_k = lapply(seq(3,end_k,2), function(x) mean(unlist(cv_function(df = df_copy,x))))
min_cv_acc_k = lapply(seq(3,end_k,2), function(x) min(unlist(cv_function(df = df_copy,x))))
sd_cv_acc_k = lapply(seq(3,end_k,2), function(x) sd(unlist(cv_function(df = df_copy,x))))
acc_df$mean_cv_accuracy = unlist(mean_cv_acc_k)
acc_df$min_cv_accuracy = unlist(min_cv_acc_k)
acc_df$sd_cv_accuracy = unlist(sd_cv_acc_k)
```

```{r}
acc_df
```

[**#Analysis**]{style="color:red"}

The above dataframe has average CV accuracy, min CV accuracy and Std. Deviation of Accuracy for each k value tried. While average CV Accuracy is a typical performance measure, we have also retained min CV accuracy which is like a lower bound on the performance of models across various validation sets, and we can also use max of this min bound to find optimal k.

We have also kept standard deviation, such that we will be able to infer the variance of accuracy across cross folds. This is helpful because the maximum CV accuracy for different values across k varies in a very close interval (around 87.5 %) -\> see graph below

#### Plot the accuracy with k (hyperparameter of knn), and find optimal k which maximizes accuracy

```{r}
ggplot(data = acc_df, aes(x = k_kknn, y = mean_cv_accuracy)) +
  geom_line() +geom_point() +
  geom_vline(xintercept = acc_df[which.max(acc_df$mean_cv_accuracy),1],
             color ="blue", size = 1.5, alpha = 0.5) +
  ggtitle(paste("max accuracy for k =",acc_df[which.max(acc_df$mean_cv_accuracy),1]))

print (paste("max accuracy for k =",acc_df[which.max(acc_df$mean_cv_accuracy),1]))
```

[**#Analysis**]{style="color:red"}

Here we see that for k = 47, we get maximum average CV accuracy of 87.74%.

However, as noted above the nuance to note here is that, for k \> 30, accuracy mostly oscillates at almost the same range with delta of 0.5. But since the standard deviation of max accuracy among cross folds is 2.45 itself, this choice of k becomes a little tricky. In other words, any k value above 30 seems appropriate, as the performance separation between the models is minimal

Another effect to note here is that, this optimal value of k, is also subject to the random sampling done to create the folds, and will change with a different random seed setting.

```{r}
acc_df[which.max(acc_df$mean_cv_accuracy),]
```

## Question 3.1

### Part 2: Train, Validation and Test on k-knn

```{r}
set.seed(77)

#split data into 70% train, 15% validation and 15% test
df_copy2 = org_cc_data
df_copy2$split_i <- sample(100, size = nrow(df_copy), replace = TRUE)

df_copy2$split = "train"
df_copy2$split[70 < df_copy2$split_i] = "valid"
df_copy2$split[85 < df_copy2$split_i] = "test"

summary_df = aggregate(df_copy2$R1,list(df_copy2$split), 
          FUN = function(x) c(mean = mean(x), proportion = length(x)/nrow(df_copy2)))
print.data.frame(summary_df)
```

Again slight deviation from expected values is seen due to random effects

```{r}
df_train = df_copy2[df_copy2$split=="train",]
dim(df_train)
df_valid = df_copy2[df_copy2$split=="valid",]
dim(df_valid)
df_test= df_copy2[df_copy2$split=="test",]
dim(df_test)
```

### Find optimal k using training on train and validation

```{r}
train_valid_func = function(df_train, df_test, k_choice){
  print (paste("For k=",k_choice))
  #split data into train and test
  print (paste("Train,Test Size =",nrow(df_train),nrow(df_test)))
  #train and get probaility predictions on test model
  test_probabilities = train_test_i(df_train,df_test, k0 = k_choice)
  #get best threshold for test probabilities
  ti = best_threshold(test_probabilities, df_test = df_test)
  print (paste("Best threshold is",ti))
  #get test accuracy
  y_act = df_test$R1
  #print (paste("Length of Test Probabilities",nrow(df_test),length(test_probabilities)))
  return (acc_func(y_act = y_act, pred_probs = test_probabilities, thresh = ti))
}
```

Test the function for a sample value

```{r}
train_valid_func(df_train = df_train, df_test = df_valid, k_choice = 20)
```

#### Run the above function for variety of k values

```{r, results='hide'}
acc_df2 <- data.frame(matrix(ncol = 0, nrow = length(seq(3,end_k,2))))
acc_df2$k_kknn = seq(3,end_k,2)
acc = lapply(seq(3,end_k,2), function(x) 
  train_valid_func(df_train = df_train, df_test = df_valid, k_choice = x))
acc_df2$acc_validation_set = unlist(acc)
```

#### Plot the accuracy values

```{r}
ggplot(data = acc_df2, aes(x = k_kknn, y = acc_validation_set)) +
  geom_line() +geom_point() +
  geom_vline(xintercept = acc_df2[which.max(acc_df2$acc_validation_set),1],
             color ="magenta", size = 1.5, alpha = 0.5) +
  ggtitle(paste("max validation accuracy for k =",acc_df2[which.max(acc_df2$acc_validation_set),1]))

print (paste("max validation accuracy for k =",acc_df2[which.max(acc_df2$acc_validation_set),1]))
```

#### Rebuild model with optimal k, with

-   Train = Train + Validation

-   Test = Test

```{r}
k_optimal = acc_df2[which.max(acc_df2$acc_validation_set),1]
acc_optimal_k = train_valid_func(df_train = rbind(df_train,df_valid), df_test = df_test, k_choice = k_optimal)
print (paste("With optimal k value, Accuracy on test =",acc_optimal_k))
```

[**#Analysis**]{style="color:red"}

Note, that both the optimal k value here and the optimal accuracy value are different than part 1 above.

The test accuracy here is the true (or best available) evaluation of the model chosen by us.

However one might also argue that the optimal k values from both the approaches lie in a similar range, since any k value \>30 can be chosen from cross validation too.

## Question 4.1

### Example 1: Market Segmentation/Retail Marketing

Clustering models are frequently used for grouping customers with similar attributes

**Objective Function (**$Y$): **Predictor Variables (**$X_i$):

-   ***Household Income***:
-   ***Household Size***:
-   ***Distance From Nearest Urban Area***:
-   ***Head of Houehold Occupation***:

## Question 4.2

```{r, results='hide'}
iris_data = read.table('./iris.txt', header = TRUE, sep='', dec='.')
head(iris_data,50)
dim(iris_data)
```

### Exploratory Data Analysis

```{r}
summary(iris_data)
```

```{r}
table(iris_data$Species)
```

```{r}
# Density Plots
my_plots = lapply(colnames(iris_data), function(x){
  p = ggplot(iris_data, aes(fill = Species)) + aes_string(x)
  if(x!='Species') {
    p = p + geom_density(alpha=.34)
  }
})

plot_grid(plotlist = my_plots)
```

[**#Analysis**]{style="color:red"} From the above distribution, we can make the following inferences: 1. Petal.Length and Petal.Width provide clear separation between the setosa vs the other classes. In clustering we expect these variables to be significantly lower in the cluster dominated by the 'setosa' classes. 2. We observe that while Sepal.Width and Sepal.Length have different central tendencies, they also show high degree of overlap between the different classes at the fringes.

```{r}
# Correlation Matrix
cor_mat = round(cor(iris_data[,1:4]), 2)
cor_mat[upper.tri(cor_mat)] <- NA
melted_mat = melt(cor_mat)
ggplot(melted_mat, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile() + geom_text(aes(Var1, Var2, label=value), color='white', size=3)
```

[**#Analysis**]{style="color:red"} From the above matrix we observed the following: 1. Petal Length and Width are highly correlated 2. Sepal Length and Petal Width are highly correlated also 3. Sepal length and Sepal Width show negligible dependence 4. Sepal width is negatively correlated petal length and width

### Modelling

```{r}
# Calculation of best 'K' by elbow graph for total sum of squares within
elbow_graph_data <- data.frame(K=integer(), within_sum_of_square=double())
for(k in seq(1, 10, by=1)){
  km.res <- kmeans(as.matrix(scale(iris_data[,1:4])), k, nstart = 25)
  elbow_graph_data[nrow(elbow_graph_data) + 1, ] = c(k, km.res$tot.withinss)
}
plot(elbow_graph_data, type='b', ylab='Total Sum of Square Within', main='Elbow Graph', col='blue')
```

[**#Analysis**]{style="color:red"} \* From the above graph, we can see that increasing the value of 'K' beyond 4 yields very low marginal benefit. \* Both 3 and 4 appear to be good values of 'K' and can be used depending on the use case. In this example, since we know that there are 3 classes, we expect 3 to be a more appropriate choice.

```{r}
# Other Method for determining best 'K' by sum of intra-cluster distance
bw_graph_data <- data.frame(K=integer(), between_sum_of_square=double())
for(k in seq(1, 10, by=1)){
  km.res <- kmeans(as.matrix(scale(iris_data[,1:4])), k, nstart = 25)
  bw_graph_data[nrow(bw_graph_data) + 1, ] = c(k, round(km.res$betweenss,2))
}
plot(bw_graph_data, type='b', ylab='Betweeen Sum of Square (Intra Cluster)', main='Betweeen Sum of Square (Intra Cluster) Graph', col='green')
```

[**#Analysis**]{style="color:red"} \* Similarly to elbow-graph, this plot also shows 3 and 4 to be good choices for 'K'.

```{r, include=FALSE}
best_km <- kmeans(as.matrix(scale(iris_data[,1:4])), 3, nstart = 25)
out<-cbind(iris_data,tagged_cluster=best_km$cluster)
out$tagged_cluster<-as.factor(out$tagged_cluster)

getBoxPlot <- function(df, y_val){
  ggplot(df, aes(x=tagged_cluster, y=y_val,fill=tagged_cluster)) + geom_boxplot(alpha=0.3) +theme(legend.position="none")  + xlab('Cluster')
}
```

```{r}
confusion_matrix <- table(iris_data$Species, best_km$cluster)
confusion_matrix
```

[**#Analysis**]{style="color:red"} From the confusion matrix, we can infer the following: \* Cluster 3 shows highest proportion of setosa class. In fact it encompasses 100% of that class. \* Cluster 2 has the highest proportion of versicolor class. \* Cluster 1 has the highest proportion of virginia class.

However in comparison to cluster 3 which is purely setosa; clusters 1 and 2 have a significant no. of data points of a different class. In other words, there is confusion between classes in clusters 1 and 2.

```{r}
getBoxPlot(out, 'Sepal.Length')
getBoxPlot(out, 'Sepal.Width')
getBoxPlot(out, 'Petal.Length')
getBoxPlot(out, 'Petal.Width')
```

[**#Analysis**]{style="color:red"} \* As we expected from the density plots and confusion matrix, Petal width and Petal Length are significant discriminator for cluster 3, which is dominated by setosa class. \* While the other two variables provide relatively lower discrimination between the clusters.

### Analysis of the misclassified points

```{r}
classifier_df = out[!out$Species=='setosa',]
classifier_df$status = ''
classifier_df$status[(classifier_df$tagged_cluster=="1" & classifier_df$Species=="versicolor")] = 'improper_versicolor'
classifier_df$status[(classifier_df$tagged_cluster=="2" & classifier_df$Species=="virginica")] = 'improper_virginica'
classifier_df$status[(classifier_df$tagged_cluster=="2" & classifier_df$Species=="versicolor")] = 'proper_versicolor'
classifier_df$status[(classifier_df$tagged_cluster=="1" & classifier_df$Species=="virginica")] = 'proper_virginica'
classifier_df$status<-as.factor(classifier_df$status)
```

```{r}
getBoxPlot(classifier_df, 'Sepal.Length')
getBoxPlot(classifier_df, 'Sepal.Width')
getBoxPlot(classifier_df, 'Petal.Length')
getBoxPlot(classifier_df, 'Petal.Width')
```

[**#Analysis**]{style="color:red"}
From the above plots, we see:
* Distribution of sepal length is very similar for improperly classified versicolor and proper virginica. Similarly, the distribution of improperly classified virginica and proper versicolor are very similar too.
* The same effect is apparent in Sepal width.
* This apparent similarity can be a probable reason for clustering to confuse between the classes.

```{r}
# Centroids of Clusters
kmeans_basic_table <- data.frame(best_km$size, best_km$centers)
kmeans_basic_table
```

### Plotting Clusters

```{r}
fviz_cluster(best_km, data=scale(iris_data[,1:2]), geom=c("point"), ellipse.type = "euclid")
```

[**#Analysis**]{style="color:red"}
* It can be seen from the plot above that cluster 1 and 2 are closely knit (i.e. intra cluster separation is less) and hence we have the confusion in clustering some points for these 2 clusters . Cluster 3 has a better separability than the other 2 and hence it is a good demarcated cluster.
* It is also apparent that cluster 3 has higher spread and hence can be split up into 2 clusters depending on the use case which also came up in our previous study (elbow graph to decide between k=3 and k=4)

### Just a visualization with 4 Cluster Space to highlight the above point

```{r}
alt_kmeans <- kmeans(as.matrix(scale(iris_data[,1:4])), 4, nstart = 25)
alt_kmeans
fviz_cluster(alt_kmeans, data=scale(iris_data[,1:2]), geom=c("point"), ellipse.type = "euclid")
```

### Accuracy in individual classes

If cluster 1 is virginica, cluster 2 is versicolor, cluster 3 is setosa

```{r}
acc_versicolor = 1 - (11/50)
acc_virginica = 1 - (14/50)
acc_setosa = 1- (0/50)

print (paste("Acc for versicolor",acc_versicolor))
print (paste("Acc for virginica",acc_virginica))
print (paste("Acc for setosa",acc_setosa))
```

[**#Analysis**]{style="color:red"}
Thus we have 100% accuracy for setosa, while for the other two flowers
we have accuracy in 70's.
