---
title: "Homework_2"
output: html_document
date: "2022-09-04"
editor_options: 
  markdown: 
    wrap: 72
---

```{r, include=FALSE}
# imports
library(ggplot2)
library(cowplot)
library(reshape2)
library(factoextra)
```

## Question 4.2

```{r, results='hide'}
iris_data = read.table('./iris.txt', header = TRUE, sep='', dec='.')
head(iris_data,50)
dim(iris_data)
```

### Exploratory Data Analysis

```{r}
summary(iris_data)
```

```{r}
table(iris_data$Species)
```

```{r}
# Density Plots
my_plots = lapply(colnames(iris_data), function(x){
  p = ggplot(iris_data, aes(fill = Species)) + aes_string(x)
  if(x!='Species') {
    p = p + geom_density(alpha=.34)
  }
})

plot_grid(plotlist = my_plots)
```

From the above distribution, we can make the following inferences: 1.
Petal.Length and Petal.Width provide clear separation between the setosa
vs the other classes. In clustering we expect these variables to be
significantly lower in the cluster dominated by the 'setosa' classes. 2.
We observe that while Sepal.Width and Sepal.Length have different
central tendencies, they also show high degree of overlap between the
different classes at the fringes.

```{r}
# Correlation Matrix
cor_mat = round(cor(iris_data[,1:4]), 2)
cor_mat[upper.tri(cor_mat)] <- NA
melted_mat = melt(cor_mat)
ggplot(melted_mat, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile() + geom_text(aes(Var1, Var2, label=value), color='white', size=3)
```

From the above matrix we observed the following: 1. Petal Length and
Width are highly correlated 2. Sepal Length and Petal Width are highly
correlated also 3. Sepal length and Sepal Width show negligible
dependence 4. Sepal width is negatively correlated petal length and
width

### Modelling

```{r}
# Calculation of best 'K'
elbow_graph_data <- data.frame(K=integer(), within_sum_of_square=double())
for(k in seq(1, 10, by=1)){
  km.res <- kmeans(as.matrix(scale(iris_data[,1:4])), k, nstart = 25)
  elbow_graph_data[nrow(elbow_graph_data) + 1, ] = c(k, km.res$tot.withinss)
}
```

```{r}
elbow_graph_data
```

```{r}
plot(elbow_graph_data, type='b', ylab='Total Sum of Square Within', main='Elbow Graph', col='blue')
```

From the above graph, we can see that increasing the value of K beyond 4
yields miminal benefit in clustering.

```{r}
# Calculation of best 'K'
bw_graph_data <- data.frame(K=integer(), between_sum_of_square=double())
for(k in seq(1, 10, by=1)){
  km.res <- kmeans(as.matrix(scale(iris_data[,1:4])), k, nstart = 25)
  bw_graph_data[nrow(bw_graph_data) + 1, ] = c(k, round(km.res$betweenss,2))
}
#options(scipen=999)
bw_graph_data
```

```{r}
plot(bw_graph_data, type='b', ylab='Betweeen Sum of Square (Intra Cluster)', main='BW SS Graph', col='green')
```

```{r}
# Calculation of best 'K'
ratio_graph_data <- data.frame(K=integer(), ratio=double())
for(k in seq(2, 10, by=1)){
  km.res <- kmeans(as.matrix(scale(iris_data[,1:4])), k, nstart = 25)
  ratio_graph_data[nrow(ratio_graph_data) + 1, ] = c(k, round(km.res$tot.withinss/km.res$betweenss,2))
}
#options(scipen=999)
ratio_graph_data
```

```{r}
plot(ratio_graph_data, type='b', ylab='Ratio Within/Between SS', main='Quality', col='red')
```

Best K is 3 because we are getting marginal benefit after that

```{r}
best_km <- kmeans(as.matrix(scale(iris_data[,1:4])), 3, nstart = 25)
out<-cbind(iris_data,tagged_cluster=best_km$cluster)
out
```

```{r}
cm <- table(iris_data$Species, best_km$cluster)
cm
```

From the confusion matrix, we can infer the following: \* Cluster 3
shows highest proportion of setosa class. In fact it encompasses 100% of
that class. \* Cluster 2 has the highest proportion of versicolor class.
\* Cluster 1 has the highest proportion of virginia class.

However in comparison to cluster 3 which is purely setosa; clusters 1
and 2 have a significant no. of data points of a different class. In
other words, there is confusion between classes in clusters 1 and 2.

```{r}
out$tagged_cluster<-as.factor(out$tagged_cluster)
ggplot(out, aes(x=tagged_cluster, y=Sepal.Length,fill=tagged_cluster)) + geom_boxplot(alpha=0.3) +theme(legend.position="none")

out$tagged_cluster<-as.factor(out$tagged_cluster)
ggplot(out, aes(x=tagged_cluster, y=Sepal.Width,fill=tagged_cluster)) + geom_boxplot(alpha=0.3) +theme(legend.position="none")

out$tagged_cluster<-as.factor(out$tagged_cluster)
ggplot(out, aes(x=tagged_cluster, y=Petal.Length,fill=tagged_cluster)) + geom_boxplot(alpha=0.3) +theme(legend.position="none")

out$tagged_cluster<-as.factor(out$tagged_cluster)
ggplot(out, aes(x=tagged_cluster, y=Petal.Width,fill=tagged_cluster)) + geom_boxplot(alpha=0.3) +theme(legend.position="none") + xlab('Cluster')
```

-   As we expected from the density plots and confusion matrix, Petal
    width and Petal Length are significant discriminator for cluster 3,
    which is dominated by setosa class.
-   While the other two variables provide relatively lower
    discrimination between the clusters.

```{r}
# function getStatusForClusterAndSpecie(row){
#   if(row$tagged_cluster=="1" & row$Species=="versicolor"){'improper_versicolor'}
#   else if(row$tagged_cluster=="2" & row$Species=="virginica"){'improper_virginica'}
#   else if()
#   
# }
```

```{r}
classifier_df = out[!out$Species=='setosa',]
classifier_df$status = ''
classifier_df$status[(classifier_df$tagged_cluster=="1" & classifier_df$Species=="versicolor")] = 'improper_versicolor'
classifier_df$status[(classifier_df$tagged_cluster=="2" & classifier_df$Species=="virginica")] = 'improper_virginica'
classifier_df$status[(classifier_df$tagged_cluster=="2" & classifier_df$Species=="versicolor")] = 'proper_versicolor'
classifier_df$status[(classifier_df$tagged_cluster=="1" & classifier_df$Species=="virginica")] = 'proper_virginica'

classifier_df

```

```{r}
classifier_df$status<-as.factor(classifier_df$status)
ggplot(classifier_df, aes(x=status, y=Sepal.Length,fill=status)) + geom_boxplot(alpha=0.3) +theme(legend.position="none")

ggplot(classifier_df, aes(x=status, y=Sepal.Width,fill=status)) + geom_boxplot(alpha=0.3) +theme(legend.position="none")

ggplot(classifier_df, aes(x=status, y=Petal.Length,fill=status)) + geom_boxplot(alpha=0.3) +theme(legend.position="none")

ggplot(classifier_df, aes(x=status, y=Petal.Width,fill=status)) + geom_boxplot(alpha=0.3) +theme(legend.position="none")
```

From the above plots, we see: \* Distribution of sepal length is very
similar for improperly classified versicolor and proper virginica.
Similarly, the distribution of improperly classified virginica and
proper versicolor are very similar too. \* The same effect is apparent
in Sepal width. \* This apparent similarity can be a probable reason for
clustering to confuse between the classes.

```{r}
out$tagged_cluster<-as.factor(out$tagged_cluster)
ggplot(out, aes(x=tagged_cluster, y=Petal.Width,fill=tagged_cluster)) + geom_boxplot(alpha=0.3) +theme(legend.position="none")
```

```{r}
# renaming predicted clusters according to categorical response variable
# renamed_clusters <- c("versicolor", "setosa", "virginica")[best_km$cluster]
```

```{r}
kmeans_basic_table <- data.frame(best_km$size, best_km$centers)
kmeans_basic_table
```

```{r}
kmeans_basic_df <- data.frame(actual_value=iris_data[,5], prediction=best_km$cluster)
kmeans_basic_df
```

```{r}
plot(scale(iris_data[,1:4]), col=best_km$cluster)
```

```{r}
# prediction_accuracy <- round(sum(kmeans_basic_df$actual_value == kmeans_basic_df$prediction) / nrow(kmeans_basic_df) * 100, 2)
# print(paste('Accuracy Acheived ->', prediction_accuracy, '%' ))
```

### Plotting Clusters

```{r}
fviz_cluster(best_km, data=scale(iris_data[,1:2]), geom=c("point"), ellipse.type = "euclid")
```

-   It can be seen from the plot above that cluster 1 and 2 are closely
    knit (i.e. intra cluster separation is less) and hence we have the
    confusion in clustering some points for these 2 clusters . Cluster 3
    has a better separability than the other 2 and hence it is a good
    demarcated cluster.

-   It is also apparent that cluster 3 has higher spread and hence can
    be split up into 2 clusters depending on the use case which also
    came up in our previous study (elbow graph to decide between k=3 and
    k=4)

***Analysing For K=4***

```{r}
alt_kmeans <- kmeans(as.matrix(scale(iris_data[,1:4])), 4, nstart = 25)
alt_kmeans
```

```{r}
cm <- table(iris_data$Species, alt_kmeans$cluster)
cm
```

```{r}
alt_kmeans$centers
```

```{r}
alt_kmeans_table_size_v_centers <- data.frame(alt_kmeans$size, alt_kmeans$centers)
alt_kmeans_table_size_v_centers
```

```{r}
alt_kmeans_row_pred <- data.frame(actual_value=iris_data[,5], prediction=alt_kmeans$cluster)
alt_kmeans_row_pred
```

```{r}
# clusters = data.frame(alt_kmeans$cluster)
# clusters[clusters$alt_kmeans.cluster==4,]
mapped_data <- data.frame(iris_data[,1:4], prediction=alt_kmeans$cluster)
```

```{r}
# Summary for Cluster 1
summary(mapped_data[mapped_data$prediction==1,1:4])
```

```{r}
# Summary for Cluster 2
summary(mapped_data[mapped_data$prediction==2,1:4])
```

```{r}
# Summary for Cluster 3
summary(mapped_data[mapped_data$prediction==3,1:4])
```

```{r}
# Summary for Cluster 4
summary(mapped_data[mapped_data$prediction==4,1:4])
```

```{r}
plot(scale(iris_data[,1:4]), col=alt_kmeans$cluster)
```

```{r}
fviz_cluster(alt_kmeans, data=scale(iris_data[,1:2]), geom=c("point"), ellipse.type = "euclid")
```

### How well the clustering fits the flower type ?

```{r}
cm <- table(iris_data$Species, best_km$cluster)
cm
```

As seen from above the total accuracy of the clustering model is:

```{r}
1-(((11+14)/nrow(iris_data)))
```

### Accuracy in individual classes

If cluster 1 is virginica, cluster 2 is versicolor, cluster 3 is setosa

```{r}
acc_versicolor = 1 - (11/50)
acc_virginica = 1 - (14/50)
acc_setosa = 1- (0/50)

print (paste("Acc for versicolor",acc_versicolor))
print (paste("Acc for virginica",acc_virginica))
print (paste("Acc for setosa",acc_setosa))
```

Thus we have 100% accuracy for setosa, while for the other two flowers
we have accuracy in 70's.
