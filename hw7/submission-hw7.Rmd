---
title: "ISYE 6501"
output:
  html_document:
    df_print: paged
  pdf_document: default
date: "2022-10-12"
---


# Submission HW4 \| Fall 22

-   Ashish Dhiman \| [ashish.dhiman\@gatech.edu](mailto:ashish.dhiman@gatech.edu){.email}

-   Abhinav Arun \| [aarun60\@gatech.edu](mailto:aarun60gatech.edu){.email}

-   Anshit Verma \| [averma373\@gatech.edu](mailto:averma373@gatech.edu){.email}

[**\*Analysis Notes are marked with Red header: #Analysis**]{style="color:red"}

## Question 11.1 

## Part 1 : Stepwise Regression

```{r, warning=FALSE, message=FALSE}
library(ggplot2)
library(qqplotr)
library(cowplot)
library(reshape2)
library(grid)
library(MASS)
library(glmnet)
#install.packages("car")
library(car)
```

```{r}
crime_data<-read.table(file = "./uscrime.txt", sep = "\t",header=TRUE)
head(crime_data)
dim(crime_data)
```


```{r}
# Fitting the full model
# The full model has 15 independent variables 
full_model <- lm(Crime ~., data = crime_data)
summary(full_model)
```

```{r}
### Data Distribution and Pairwise Corelation
my_plots <- lapply(names(crime_data), function(var_x){
  p <- 
    ggplot(crime_data) +
    aes_string(var_x)
  if(var_x %in% list("So")) {
    p <- p + geom_bar()
  } else {
    p <- p + geom_density()
  } 
})
plot_grid(plotlist = my_plots)
### Correlation
cormat <- round(cor(crime_data),2)
cormat[upper.tri(cormat)] <- NA
melted_cormat <- melt(cormat)
# plotting the correlation heatmap
ggplot(data = melted_cormat, aes(x=Var1, y=Var2,
                                   fill=value)) +
geom_tile() +
geom_text(aes(Var1, Var2, label = value),
          color = "white", size = 3)
```

```{r}
#The "maximal" model is a linear regression model which assumes independent model errors and includes only main effects for the predictor variables.

#The "maximal" model is used as input for the stepAIC() function

# Stepwise regression model to choose the best model by AIC in a stepwise algorithm (shown with iterations)
step_model <- stepAIC(full_model, direction = "both")
summary(step_model)

```

```{r}
#Final model at the end of stepwise regression model
final_model<-stepAIC(full_model, direction = "both",trace=FALSE)
summary(final_model)

# Computing the Variance Inflation factor to see if multicolinearity issue exists in the final model
vif(final_model)

```

# Observations/Inferences : 

1. The initial full model has 15 independent predictor variables.  Whereas , our final model output of the Stepwise Regression Model has 8 important predictor variables that minimizes the AIC value . These variables are evaluated on using the combination of forward selection and backward elimination methods iteratively. The lower the AIC value , the better the model as AIC value estimates the relative amount of information lost by a given model in an effort to estimate the actual model (Info Theory).

2. Another point to note is that the Multiple R-squared of the final model is 0.789 which is lower than the initial model which has a value of 0.8031 but this is the pitfall of R squared value i.e. It will always increases with the increase in the # of variables without any penalty/adjustment for model complexity/overfitting unlike AIC or Adjusted R-squared.

3. If we observe Adjusted R squared value , it is higher for our final model with 8 independent predictor variables , thus the final model is validated to be better than the initial model on the basis of Adjusted R squared value too.

![](adjs-R.png)

Based on the above formula , we can infer that the Adjusted R squared value decreases with the increase in # of independent variables(k) until and unless if there is a significant increase in the R squared value on adding a new variable. Thus both AIC and Adjusted R squared provide a technique for selecting simpler model which prevents overfitting.

4. Also , based on the correlation plot it could be seen that variables Wealth and Income Inequality are 2 highly correlated variables (-ve correlation), hence we should pick only 1 variable out of the 2. Same goes for Po1 and Po2. So , if we see our final model , it has just Ineq (Income Inequality) and Po1 (one out of each pair of correlated variables ) and thus our stepwise regression model handles it implicitly.

5. Since , the Variance Inflation factor for all the predictor variables in the final output is less than 5 , we can infer that the problem of multicollinearity also does not reside in our final model.

## Part 2: Lasso Regression

```{r}
crime_df = read.table("./uscrime.txt", sep="\t", header= TRUE)
dim(crime_df)
names(crime_df)
```

```{r}
x = as.matrix(crime_df[,names(crime_df) != "Crime"])
y = as.matrix(crime_df[,"Crime"])
dim(x)
dim(y)
```

```{r}
fit <- glmnet(x, y, alpha = 1, nlambda = 20, standardize = TRUE)
```

```{r}
print(fit)
```

```{r}
plot(fit, xvar = "lambda", label = TRUE)
```

```{r}
cvfit <- cv.glmnet(x = x, y = y, alpha = 1, nlambda = 30, standardize = TRUE, type.measure = "mse", nfolds = 5)
```

Because we have only 47 points in the data, we don't want fit too many models during the CV stage

```{r}
plot(cvfit)
```

Above plot gives CV error against values of lambda. With increasing Lambda the weight of regularization factor increases, and therefore the model is encouraged, to select lower number of predictors.

```{r}
print (cvfit)
```

Two special values along the ðœ† sequence are indicated by the vertical dotted lines. `lambda.min` is the value of Î» that gives minimum mean cross-validated error, while `lambda.1se` is the value of Î» that gives the most regularized model such that the cross-validated error is within one standard error of the minimum.

```{r}
coef(cvfit, s = "lambda.min")
```

```{r}
coef(cvfit, s = "lambda.1se")
```

```{r}
cvfit$lambda.1se
```

```{r}
fit_1se <- glmnet(x, y, alpha = 1, lambda = cvfit$lambda.1se, standardize = TRUE)
coef(fit_1se)

fit_min <- glmnet(x, y, alpha = 1, lambda = cvfit$lambda.min, standardize = TRUE)
coef(fit_min)
```

```{r}
predict_min = predict(fit_min, newx = x, s = "lambda.min")
predict_1se = predict(fit_1se, newx = x, s = "lambda.1se")
```

```{r}
rmse_func = function(true,predicted){
  (mean((true-predicted)^2))**0.5
}
```

### RMSE for 

```{r}
rmse_func(true=y,predicted = predict_min)
```

```{r}
rmse_func(true=y,predicted = predict_1se)
```

## Part 3 : ElasticNet Regression

* Elastic net is a combination of both Lasso and Ridge regression. It uses both L1 and L2 penalty to restrict the regression solution.
* It involves two hyperparameters - Lambda and Alpha.
* Alpha is used to give direction between Ridge and Lasso. Alpha = 0 is Ridge and Alpha = 1 is Lasso.
* Lambda is amount of penalty in the regression.

```{r}
data = read.table("./uscrime.txt", sep="\t", header=TRUE)
head(data)
```

```{r}
x = as.matrix(data[, 1:ncol(data)-1])
y = data[,"Crime"]
```

```{r}
x = scale(x)
```

### Train Test Split

```{r}
n=nrow(data)
train_rows <- sample(1:n, .8*n, replace = F)

x.train <- x[train_rows,]
y.train <- y[train_rows]

x.test <- x[-train_rows,]
y.test <- y[-train_rows]
```

* Splitting the data to get a value of MSE for deciding a value of alpha.

### Estimating Lambda and Alpha - hyperparameters

```{r}
models <- list()
for (i in 0:20) {
  name <- paste0("alpha", i/20)
  
  models[[name]] <-
    cv.glmnet(as.matrix(x.train), as.matrix(y.train), alpha=i/20, family="gaussian", nfold = 5)
}
```

* We are fixing the value of alpha and using cross validation for find the value of lambda which minimizes the value of MSE - our metric for validation.
* We find a minimum value of lambda which minimizes the MSE and another value of lambda one standard deviation apart as it is more practical. We are accepting some performance loss and incorporating more bias in the model.


```{r}
results <- data.frame()
for (i in 0:20) {
  name <- paste0("alpha", i/20)
  
  ## Use each model to predict 'y' given the Testing dataset
  predicted <- predict(models[[name]], 
                       s=models[[name]]$lambda.1se, newx=x.test)
  
  ## Calculate the Mean Squared Error...
  mse <- mean((y.test - predicted)^2)
  
  ## Store the results
  temp <- data.frame(alpha=i/20, mse=mse, name=name)
  results <- rbind(results, temp)
}
```


* After we have selected our lambda, we predict on the test data for getting the value of alpha.
* Alpha is the parameter for deciding between ridge (L2 norm) penalty and lasso (L1 penalty) penalty.


```{r}
print(results)
```

```{r}
plot(results$alpha, results$mse)
```

```{r}
results[which.min(results$mse),]
```

It can be observed from the above outputs that the value of alpha at which we get the best fit and lowest value of MSE is at alpha = 0.3.

--------  DO NOT INCLUDE AFTER THIS LINE ---------

```{r}
library(MASS)
library(caret)
```

```{r}
custom <- trainControl(method = "repeatedcv", number = 10, repeats = 5, verboseIter = TRUE)
```

```{r}
set.seed(1234)
en <- train(x = x.train, y = y.train,
            method='glmnet',
            tuneGrid =expand.grid(alpha=seq(0,1,length=10),
                                  lambda = seq(0.0001,0.2,length=5)),
            trControl=custom)
```

```{r}
en
```

```{r}
mean(en$resample$RMSE)
```


```{r}
plot(en, main = "Elastic Net Regression")
```

```{r}
plot(varImp(en,scale=TRUE))
```